{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#coding=utf-8\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import numpy as np \n",
    "from keras import *\n",
    "from keras.models import Sequential  \n",
    "from keras.layers import *\n",
    "from keras.utils.np_utils import to_categorical  \n",
    "from keras.preprocessing.image import img_to_array  \n",
    "from keras.callbacks import ModelCheckpoint  \n",
    "from Models.utils import MaxUnpooling2D,MaxPoolingWithArgmax2D\n",
    "from sklearn.preprocessing import LabelEncoder  \n",
    "from PIL import Image  \n",
    "import matplotlib.pyplot as plt  \n",
    "import cv2\n",
    "import random\n",
    "import os\n",
    "from tqdm import tqdm  \n",
    "from keras import backend as K \n",
    "K.set_image_dim_ordering('th')\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import time\n",
    "import gdal\n",
    "seed = 7  \n",
    "np.random.seed(seed)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data for training  \n",
    "def generateData(batch_size,img_w,img_h,n_label,image_names=[],label_names=[]): \n",
    "    print ('gen-Sub-Image-Data...')\n",
    "    image_filepath ='D:\\Python\\seg-data\\data_MB/'\n",
    "    batch_num=0\n",
    "    while True:   \n",
    "        bs=batch_size\n",
    "        \n",
    "        dataset = gdal.Open(image_filepath+image_names[batch_num%len(image_names)])\n",
    "        im_width = dataset.RasterXSize #栅格矩阵的列数\n",
    "        im_height = dataset.RasterYSize #栅格矩阵的行数\n",
    "        image_data = dataset.ReadAsArray(0,0,im_width,im_height)\n",
    "        label_data=cv2.imread(image_filepath+label_names[batch_num%len(image_names)],cv2.IMREAD_GRAYSCALE)\n",
    "        train_data = []  \n",
    "        train_label =  []  \n",
    "        for i in range(bs):\n",
    "            random_width = random.randint(0, im_width - img_w - 1)\n",
    "            random_height = random.randint(0, im_height - img_h - 1)\n",
    "            bands_roi=[]\n",
    "            for band in image_data :\n",
    "                band_roi = band[random_height: random_height + img_h, random_width: random_width + img_w]\n",
    "                bands_roi.append(band_roi)\n",
    "            data_roi=bands_roi\n",
    "            #to_categorical(train_label, num_classes=n_label)  \n",
    "            label_roi = to_categorical((label_data[random_height: random_height + img_h, random_width: random_width + img_w]).flatten(), num_classes=n_label)\n",
    "            train_data.append( data_roi)  \n",
    "            train_label.append(label_roi)\n",
    "        #yield(np.array(train_data).shape,np.array(train_label).shape)    \n",
    "        yield(np.array(train_data),np.array(train_label))\n",
    "        batch_num=batch_num+1\n",
    "#image_names_set=['test.tif']\n",
    "#label_names_set=['test_label.png']\n",
    "#for i in(generateData(8,256,256,2,image_names_set,label_names_set)):\n",
    "#    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SegNet(\n",
    "        input_shape=(4,256,256),\n",
    "        n_labels=2,\n",
    "        kernel=3,\n",
    "        pool_size=(2, 2),\n",
    "        output_mode=\"softmax\"):\n",
    "    # encoder\n",
    "    inputs = Input(shape=input_shape)\n",
    "    img_w=input_shape[1]\n",
    "    img_h=input_shape[2] \n",
    "    conv_1 = Convolution2D(64, (kernel, kernel), padding=\"same\")(inputs)\n",
    "    conv_1 = BatchNormalization()(conv_1)\n",
    "    conv_1 = Activation(\"relu\")(conv_1)\n",
    "    conv_2 = Convolution2D(64, (kernel, kernel), padding=\"same\")(conv_1)\n",
    "    conv_2 = BatchNormalization()(conv_2)\n",
    "    conv_2 = Activation(\"relu\")(conv_2)\n",
    "\n",
    "    pool_1, mask_1 = MaxPoolingWithArgmax2D(pool_size)(conv_2)\n",
    "\n",
    "    conv_3 = Convolution2D(128, (kernel, kernel), padding=\"same\")(pool_1)\n",
    "    conv_3 = BatchNormalization()(conv_3)\n",
    "    conv_3 = Activation(\"relu\")(conv_3)\n",
    "    conv_4 = Convolution2D(128, (kernel, kernel), padding=\"same\")(conv_3)\n",
    "    conv_4 = BatchNormalization()(conv_4)\n",
    "    conv_4 = Activation(\"relu\")(conv_4)\n",
    "\n",
    "    pool_2, mask_2 = MaxPoolingWithArgmax2D(pool_size)(conv_4)\n",
    "\n",
    "    conv_5 = Convolution2D(256, (kernel, kernel), padding=\"same\")(pool_2)\n",
    "    conv_5 = BatchNormalization()(conv_5)\n",
    "    conv_5 = Activation(\"relu\")(conv_5)\n",
    "    conv_6 = Convolution2D(256, (kernel, kernel), padding=\"same\")(conv_5)\n",
    "    conv_6 = BatchNormalization()(conv_6)\n",
    "    conv_6 = Activation(\"relu\")(conv_6)\n",
    "    conv_7 = Convolution2D(256, (kernel, kernel), padding=\"same\")(conv_6)\n",
    "    conv_7 = BatchNormalization()(conv_7)\n",
    "    conv_7 = Activation(\"relu\")(conv_7)\n",
    "\n",
    "    pool_3, mask_3 = MaxPoolingWithArgmax2D(pool_size)(conv_7)\n",
    "\n",
    "    conv_8 = Convolution2D(512, (kernel, kernel), padding=\"same\")(pool_3)\n",
    "    conv_8 = BatchNormalization()(conv_8)\n",
    "    conv_8 = Activation(\"relu\")(conv_8)\n",
    "    conv_9 = Convolution2D(512, (kernel, kernel), padding=\"same\")(conv_8)\n",
    "    conv_9 = BatchNormalization()(conv_9)\n",
    "    conv_9 = Activation(\"relu\")(conv_9)\n",
    "    conv_10 = Convolution2D(512, (kernel, kernel), padding=\"same\")(conv_9)\n",
    "    conv_10 = BatchNormalization()(conv_10)\n",
    "    conv_10 = Activation(\"relu\")(conv_10)\n",
    "\n",
    "    pool_4, mask_4 = MaxPoolingWithArgmax2D(pool_size)(conv_10)\n",
    "\n",
    "    conv_11 = Convolution2D(512, (kernel, kernel), padding=\"same\")(pool_4)\n",
    "    conv_11 = BatchNormalization()(conv_11)\n",
    "    conv_11 = Activation(\"relu\")(conv_11)\n",
    "    conv_12 = Convolution2D(512, (kernel, kernel), padding=\"same\")(conv_11)\n",
    "    conv_12 = BatchNormalization()(conv_12)\n",
    "    conv_12 = Activation(\"relu\")(conv_12)\n",
    "    conv_13 = Convolution2D(512, (kernel, kernel), padding=\"same\")(conv_12)\n",
    "    conv_13 = BatchNormalization()(conv_13)\n",
    "    conv_13 = Activation(\"relu\")(conv_13)\n",
    "\n",
    "    pool_5, mask_5 = MaxPoolingWithArgmax2D(pool_size)(conv_13)\n",
    "    print(\"Build enceder done..\\n\")\n",
    "\n",
    "    # decoder\n",
    "\n",
    "    unpool_1 = MaxUnpooling2D(pool_size)([pool_5, mask_5])\n",
    "\n",
    "    conv_14 = Convolution2D(512, (kernel, kernel), padding=\"same\")(unpool_1)\n",
    "    conv_14 = BatchNormalization()(conv_14)\n",
    "    conv_14 = Activation(\"relu\")(conv_14)\n",
    "    conv_15 = Convolution2D(512, (kernel, kernel), padding=\"same\")(conv_14)\n",
    "    conv_15 = BatchNormalization()(conv_15)\n",
    "    conv_15 = Activation(\"relu\")(conv_15)\n",
    "    conv_16 = Convolution2D(512, (kernel, kernel), padding=\"same\")(conv_15)\n",
    "    conv_16 = BatchNormalization()(conv_16)\n",
    "    conv_16 = Activation(\"relu\")(conv_16)\n",
    "\n",
    "    unpool_2 = MaxUnpooling2D(pool_size)([conv_16, mask_4])\n",
    "\n",
    "    conv_17 = Convolution2D(512, (kernel, kernel), padding=\"same\")(unpool_2)\n",
    "    conv_17 = BatchNormalization()(conv_17)\n",
    "    conv_17 = Activation(\"relu\")(conv_17)\n",
    "    conv_18 = Convolution2D(512, (kernel, kernel), padding=\"same\")(conv_17)\n",
    "    conv_18 = BatchNormalization()(conv_18)\n",
    "    conv_18 = Activation(\"relu\")(conv_18)\n",
    "    conv_19 = Convolution2D(256, (kernel, kernel), padding=\"same\")(conv_18)\n",
    "    conv_19 = BatchNormalization()(conv_19)\n",
    "    conv_19 = Activation(\"relu\")(conv_19)\n",
    "\n",
    "    unpool_3 = MaxUnpooling2D(pool_size)([conv_19, mask_3])\n",
    "\n",
    "    conv_20 = Convolution2D(256, (kernel, kernel), padding=\"same\")(unpool_3)\n",
    "    conv_20 = BatchNormalization()(conv_20)\n",
    "    conv_20 = Activation(\"relu\")(conv_20)\n",
    "    conv_21 = Convolution2D(256, (kernel, kernel), padding=\"same\")(conv_20)\n",
    "    conv_21 = BatchNormalization()(conv_21)\n",
    "    conv_21 = Activation(\"relu\")(conv_21)\n",
    "    conv_22 = Convolution2D(128, (kernel, kernel), padding=\"same\")(conv_21)\n",
    "    conv_22 = BatchNormalization()(conv_22)\n",
    "    conv_22 = Activation(\"relu\")(conv_22)\n",
    "\n",
    "    unpool_4 = MaxUnpooling2D(pool_size)([conv_22, mask_2])\n",
    "\n",
    "    conv_23 = Convolution2D(128, (kernel, kernel), padding=\"same\")(unpool_4)\n",
    "    conv_23 = BatchNormalization()(conv_23)\n",
    "    conv_23 = Activation(\"relu\")(conv_23)\n",
    "    conv_24 = Convolution2D(64, (kernel, kernel), padding=\"same\")(conv_23)\n",
    "    conv_24 = BatchNormalization()(conv_24)\n",
    "    conv_24 = Activation(\"relu\")(conv_24)\n",
    "\n",
    "    unpool_5 = MaxUnpooling2D(pool_size)([conv_24, mask_1])\n",
    "\n",
    "    conv_25 = Convolution2D(64, (kernel, kernel), padding=\"same\")(unpool_5)\n",
    "    conv_25 = BatchNormalization()(conv_25)\n",
    "    conv_25 = Activation(\"relu\")(conv_25)\n",
    "\n",
    "    conv_26 = Convolution2D(n_labels, (1, 1), padding=\"valid\")(conv_25)\n",
    "    conv_26 = BatchNormalization()(conv_26)\n",
    "   \n",
    "    #conv_26  = Reshape((-1,img_w*img_h,n_labels))(conv_26 )\n",
    "\n",
    "    outputs = Activation(output_mode)(conv_26)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs, name=\"SegNet\")\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='sgd',metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build enceder done..\n",
      "\n",
      "the number of train data is 1000 250\n",
      "the number of val data is 200 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\chao\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:23: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(generator=<generator..., steps_per_epoch=250, epochs=1, verbose=1, validation_data=<generator..., validation_steps=50, callbacks=[<keras.ca..., max_queue_size=1)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "gen-Sub-Image-Data...gen-Sub-Image-Data...\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected activation_26 to have 4 dimensions, but got array with shape (4, 65536, 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-bafcb0a3f5f3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-bafcb0a3f5f3>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m                             \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalid_numb\u001b[0m\u001b[1;33m//\u001b[0m\u001b[0mBS\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallable\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m                             max_q_size=1)  \n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;31m# plot the training loss and accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chao\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chao\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1418\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1420\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chao\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[0;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m                                             class_weight=class_weight)\n\u001b[0m\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chao\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1209\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1210\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1211\u001b[1;33m             class_weight=class_weight)\n\u001b[0m\u001b[0;32m   1212\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1213\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1.\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chao\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 789\u001b[1;33m                 exception_prefix='target')\n\u001b[0m\u001b[0;32m    790\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[1;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chao\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    126\u001b[0m                         \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    129\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking target: expected activation_26 to have 4 dimensions, but got array with shape (4, 65536, 2)"
     ]
    }
   ],
   "source": [
    "def train(): \n",
    "    EPOCHS = 1\n",
    "    BS = 4\n",
    "    img_w = 256  \n",
    "    img_h = 256  \n",
    "    model = SegNet() \n",
    "    \n",
    "    modelcheck = ModelCheckpoint('..\\..\\Python\\seg-data/model/4Bands-'+time.strftime(f'%Y-%m-%d-%a-%H-%M-%S',time.localtime(time.time()))+'.h5',monitor='val_acc',save_best_only=True,mode='max')  \n",
    "    \n",
    "    callable = [modelcheck]  \n",
    "    \n",
    "    train_numb = 1000\n",
    "    valid_numb = 200\n",
    "    print (\"the number of train data is\",train_numb,train_numb//BS)  \n",
    "    print (\"the number of val data is\",valid_numb,valid_numb//BS)\n",
    "    H = model.fit_generator(generator=generateData(BS,img_w,img_h,2,['test.tif'],['test_label.png']),\n",
    "                            steps_per_epoch=train_numb//BS,\n",
    "                            epochs=EPOCHS,\n",
    "                            verbose=1,\n",
    "                            validation_data=generateData(BS,img_w,img_h,2,['test.tif'],['test_label.png']),\n",
    "                            validation_steps=valid_numb//BS,\n",
    "                            callbacks=callable,\n",
    "                            max_q_size=1)  \n",
    "\n",
    "    # plot the training loss and accuracy\n",
    "    plt.style.use(\"ggplot\")\n",
    "    plt.figure()\n",
    "    N = EPOCHS\n",
    "    plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n",
    "    plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n",
    "    plt.plot(np.arange(0, N), H.history[\"acc\"], label=\"train_acc\")\n",
    "    plt.plot(np.arange(0, N), H.history[\"val_acc\"], label=\"val_acc\")\n",
    "    plt.title(\"Training Loss and Accuracy on SegNet Satellite Seg\")\n",
    "    plt.xlabel(\"Epoch #\")\n",
    "    plt.ylabel(\"Loss/Accuracy\")\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    plt.savefig(\"plot.png\")\n",
    "\n",
    "\n",
    "\n",
    "def args_parse():\n",
    "    # construct the argument parse and parse the arguments\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument(\"-a\", \"--augment\",help=\"using data augment or not\",\n",
    "                    action=\"store_true\", default=False)\n",
    "    ap.add_argument(\"-m\", \"--model\", required=True,default='model/'+time.strftime(f'%Y-%m-%d %a %H:%M:%S',time.localtime(time.time()))+'.h5'\n",
    "                    ,help=\"path to output model\")\n",
    "    ap.add_argument(\"-p\", \"--plot\", type=str, default=\"plot.png\",\n",
    "                    help=\"path to output accuracy/loss plot\")\n",
    "    args = vars(ap.parse_args()) \n",
    "    return args\n",
    "\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "m = SegNet()\n",
    "from keras.utils import plot_model\n",
    "plot_model(m, show_shapes=True, to_file='model_segnet.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
