{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#coding=utf-8\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import numpy as np \n",
    "from keras import *\n",
    "from keras.models import Sequential  \n",
    "from keras.layers import *\n",
    "from keras.layers import Input\n",
    "from keras.utils.np_utils import to_categorical  \n",
    "from keras.preprocessing.image import img_to_array  \n",
    "from keras.callbacks import ModelCheckpoint ,TensorBoard\n",
    "from Models import *\n",
    "from Models.utils import *\n",
    "from sklearn.preprocessing import LabelEncoder  \n",
    "from PIL import Image  \n",
    "import matplotlib.pyplot as plt  \n",
    "import cv2\n",
    "import random\n",
    "import os\n",
    "from tqdm import tqdm  \n",
    "from keras import backend as K \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import time\n",
    "import gdal\n",
    "seed = 7  \n",
    "np.random.seed(seed)  \n",
    "# data for training  \n",
    "from keras.applications import vgg16\n",
    " \n",
    "def generateDataTF(batch_size,img_w,img_h,n_label,image_names=[],label_names=[]): \n",
    "    print ('gen-Sub-Image-Data...')\n",
    "    image_filepath ='D:\\Python\\seg-data\\data_MB/'\n",
    "    batch_num=0\n",
    "    while True:   \n",
    "        bs=batch_size\n",
    "        \n",
    "        dataset = gdal.Open(image_filepath+image_names[batch_num%len(image_names)])\n",
    "        im_width = dataset.RasterXSize #栅格矩阵的列数\n",
    "        im_height = dataset.RasterYSize #栅格矩阵的行数\n",
    "        #print(im_width ,im_height)\n",
    "        label_data=cv2.imread(image_filepath+label_names[batch_num%len(image_names)],cv2.IMREAD_GRAYSCALE)\n",
    "        #yield(label_data.shape)\n",
    "        train_data = []  \n",
    "        train_label =  []  \n",
    "        i=0\n",
    "        while (bs-i)!=0:\n",
    "            random_width = random.randint(0, im_width - img_w - 1)\n",
    "            random_height = random.randint(0, im_height - img_h - 1)\n",
    "            tif_roi=dataset.ReadAsArray(random_width,random_height,img_w,img_h)\n",
    "            if (np.sum(tif_roi[0]==0)/(im_width*im_height))<0.5:\n",
    "                data_roi=cv2.merge(tif_roi)  \n",
    "                label_roi = to_categorical((label_data[random_height: random_height + img_h , random_width: random_width + img_w]).flatten(), num_classes=n_label)\n",
    "                train_data.append( data_roi)  \n",
    "                train_label.append(label_roi)\n",
    "                i=i+1\n",
    "                #yield(random_width,img_w,random_height,img_h)\n",
    "                #yield(np.array(data_roi).shape,np.array(label_roi).shape)    \n",
    "        #yield(np.array(train_data).shape,np.array(train_label).shape)    \n",
    "        yield(np.array(train_data),np.array(train_label))\n",
    "        batch_num=batch_num+1\n",
    "#image_names_set=['test.tif']\n",
    "#label_names_set=['test_label.png']\n",
    "#for i in(generateDataTF(8,256,256,2,image_names_set,label_names_set)):\n",
    "#    print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def FCN32(\n",
    "          input_shape=(256,256,4),\n",
    "          n_labels=2,\n",
    "          kernel=3,\n",
    "          pool_size=(2, 2),\n",
    "          output_mode=\"softmax\"):\n",
    "    nClasses=n_labels\n",
    "    input_height=input_shape[0]\n",
    "    input_width=input_shape[1]\n",
    "    img_input = Input(shape=(input_height, input_width, input_shape[2]))\n",
    "    assert input_height % 32 == 0\n",
    "    assert input_width % 32 == 0\n",
    "\n",
    "\n",
    "    model = vgg16.VGG16(\n",
    "        include_top=False,\n",
    "        weights=None, input_tensor=img_input)\n",
    "    assert isinstance(model, Model)\n",
    "\n",
    "    o = Conv2D(\n",
    "        filters=4096,\n",
    "        kernel_size=(\n",
    "            7,\n",
    "            7),\n",
    "        padding=\"same\",\n",
    "        activation=\"relu\",\n",
    "        name=\"fc6\")(\n",
    "            model.output)\n",
    "    o = Dropout(rate=0.5)(o)\n",
    "    o = Conv2D(\n",
    "        filters=4096,\n",
    "        kernel_size=(\n",
    "            1,\n",
    "            1),\n",
    "        padding=\"same\",\n",
    "        activation=\"relu\",\n",
    "        name=\"fc7\")(o)\n",
    "    o = Dropout(rate=0.5)(o)\n",
    "\n",
    "    o = Conv2D(filters=nClasses, kernel_size=(1, 1), padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "               name=\"score_fr\")(o)\n",
    "\n",
    "    o = Conv2DTranspose(filters=nClasses, kernel_size=(32, 32), strides=(32, 32), padding=\"valid\", activation=None,\n",
    "                        name=\"score2\")(o)\n",
    "\n",
    "    o = Reshape((-1, nClasses))(o)\n",
    "    o = Activation(\"softmax\")(o)\n",
    "\n",
    "    fcn = Model(inputs=img_input, outputs=o)\n",
    "    # mymodel.summary()\n",
    "    return fcn\n",
    "#m = FCN32()\n",
    "#m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SegNet(\n",
    "        input_shape=(256,256,4),\n",
    "        n_labels=2,\n",
    "        kernel=3,\n",
    "        pool_size=(2, 2),\n",
    "        output_mode=\"softmax\"):\n",
    "    # encoder\n",
    "    inputs = Input(shape=input_shape)\n",
    "    img_w=input_shape[1]\n",
    "    img_h=input_shape[2] \n",
    "    conv_1 = Convolution2D(64, (kernel, kernel), padding=\"same\")(inputs)\n",
    "    conv_1 = BatchNormalization()(conv_1)\n",
    "    conv_1 = Activation(\"relu\")(conv_1)\n",
    "    conv_2 = Convolution2D(64, (kernel, kernel), padding=\"same\")(conv_1)\n",
    "    conv_2 = BatchNormalization()(conv_2)\n",
    "    conv_2 = Activation(\"relu\")(conv_2)\n",
    "\n",
    "    pool_1, mask_1 = MaxPoolingWithArgmax2D(pool_size)(conv_2)\n",
    "\n",
    "    conv_3 = Convolution2D(128, (kernel, kernel), padding=\"same\")(pool_1)\n",
    "    conv_3 = BatchNormalization()(conv_3)\n",
    "    conv_3 = Activation(\"relu\")(conv_3)\n",
    "    conv_4 = Convolution2D(128, (kernel, kernel), padding=\"same\")(conv_3)\n",
    "    conv_4 = BatchNormalization()(conv_4)\n",
    "    conv_4 = Activation(\"relu\")(conv_4)\n",
    "\n",
    "    pool_2, mask_2 = MaxPoolingWithArgmax2D(pool_size)(conv_4)\n",
    "\n",
    "    conv_5 = Convolution2D(256, (kernel, kernel), padding=\"same\")(pool_2)\n",
    "    conv_5 = BatchNormalization()(conv_5)\n",
    "    conv_5 = Activation(\"relu\")(conv_5)\n",
    "    conv_6 = Convolution2D(256, (kernel, kernel), padding=\"same\")(conv_5)\n",
    "    conv_6 = BatchNormalization()(conv_6)\n",
    "    conv_6 = Activation(\"relu\")(conv_6)\n",
    "    conv_7 = Convolution2D(256, (kernel, kernel), padding=\"same\")(conv_6)\n",
    "    conv_7 = BatchNormalization()(conv_7)\n",
    "    conv_7 = Activation(\"relu\")(conv_7)\n",
    "\n",
    "    pool_3, mask_3 = MaxPoolingWithArgmax2D(pool_size)(conv_7)\n",
    "\n",
    "    conv_8 = Convolution2D(512, (kernel, kernel), padding=\"same\")(pool_3)\n",
    "    conv_8 = BatchNormalization()(conv_8)\n",
    "    conv_8 = Activation(\"relu\")(conv_8)\n",
    "    conv_9 = Convolution2D(512, (kernel, kernel), padding=\"same\")(conv_8)\n",
    "    conv_9 = BatchNormalization()(conv_9)\n",
    "    conv_9 = Activation(\"relu\")(conv_9)\n",
    "    conv_10 = Convolution2D(512, (kernel, kernel), padding=\"same\")(conv_9)\n",
    "    conv_10 = BatchNormalization()(conv_10)\n",
    "    conv_10 = Activation(\"relu\")(conv_10)\n",
    "\n",
    "    pool_4, mask_4 = MaxPoolingWithArgmax2D(pool_size)(conv_10)\n",
    "\n",
    "    conv_11 = Convolution2D(512, (kernel, kernel), padding=\"same\")(pool_4)\n",
    "    conv_11 = BatchNormalization()(conv_11)\n",
    "    conv_11 = Activation(\"relu\")(conv_11)\n",
    "    conv_12 = Convolution2D(512, (kernel, kernel), padding=\"same\")(conv_11)\n",
    "    conv_12 = BatchNormalization()(conv_12)\n",
    "    conv_12 = Activation(\"relu\")(conv_12)\n",
    "    conv_13 = Convolution2D(512, (kernel, kernel), padding=\"same\")(conv_12)\n",
    "    conv_13 = BatchNormalization()(conv_13)\n",
    "    conv_13 = Activation(\"relu\")(conv_13)\n",
    "\n",
    "    pool_5, mask_5 = MaxPoolingWithArgmax2D(pool_size)(conv_13)\n",
    "    print(\"Build enceder done..\")\n",
    "\n",
    "    # decoder\n",
    "\n",
    "    unpool_1 = MaxUnpooling2D(pool_size)([pool_5, mask_5])\n",
    "\n",
    "    conv_14 = Convolution2D(512, (kernel, kernel), padding=\"same\")(unpool_1)\n",
    "    conv_14 = BatchNormalization()(conv_14)\n",
    "    conv_14 = Activation(\"relu\")(conv_14)\n",
    "    conv_15 = Convolution2D(512, (kernel, kernel), padding=\"same\")(conv_14)\n",
    "    conv_15 = BatchNormalization()(conv_15)\n",
    "    conv_15 = Activation(\"relu\")(conv_15)\n",
    "    conv_16 = Convolution2D(512, (kernel, kernel), padding=\"same\")(conv_15)\n",
    "    conv_16 = BatchNormalization()(conv_16)\n",
    "    conv_16 = Activation(\"relu\")(conv_16)\n",
    "\n",
    "    unpool_2 = MaxUnpooling2D(pool_size)([conv_16, mask_4])\n",
    "\n",
    "    conv_17 = Convolution2D(512, (kernel, kernel), padding=\"same\")(unpool_2)\n",
    "    conv_17 = BatchNormalization()(conv_17)\n",
    "    conv_17 = Activation(\"relu\")(conv_17)\n",
    "    conv_18 = Convolution2D(512, (kernel, kernel), padding=\"same\")(conv_17)\n",
    "    conv_18 = BatchNormalization()(conv_18)\n",
    "    conv_18 = Activation(\"relu\")(conv_18)\n",
    "    conv_19 = Convolution2D(256, (kernel, kernel), padding=\"same\")(conv_18)\n",
    "    conv_19 = BatchNormalization()(conv_19)\n",
    "    conv_19 = Activation(\"relu\")(conv_19)\n",
    "\n",
    "    unpool_3 = MaxUnpooling2D(pool_size)([conv_19, mask_3])\n",
    "\n",
    "    conv_20 = Convolution2D(256, (kernel, kernel), padding=\"same\")(unpool_3)\n",
    "    conv_20 = BatchNormalization()(conv_20)\n",
    "    conv_20 = Activation(\"relu\")(conv_20)\n",
    "    conv_21 = Convolution2D(256, (kernel, kernel), padding=\"same\")(conv_20)\n",
    "    conv_21 = BatchNormalization()(conv_21)\n",
    "    conv_21 = Activation(\"relu\")(conv_21)\n",
    "    conv_22 = Convolution2D(128, (kernel, kernel), padding=\"same\")(conv_21)\n",
    "    conv_22 = BatchNormalization()(conv_22)\n",
    "    conv_22 = Activation(\"relu\")(conv_22)\n",
    "\n",
    "    unpool_4 = MaxUnpooling2D(pool_size)([conv_22, mask_2])\n",
    "\n",
    "    conv_23 = Convolution2D(128, (kernel, kernel), padding=\"same\")(unpool_4)\n",
    "    conv_23 = BatchNormalization()(conv_23)\n",
    "    conv_23 = Activation(\"relu\")(conv_23)\n",
    "    conv_24 = Convolution2D(64, (kernel, kernel), padding=\"same\")(conv_23)\n",
    "    conv_24 = BatchNormalization()(conv_24)\n",
    "    conv_24 = Activation(\"relu\")(conv_24)\n",
    "\n",
    "    unpool_5 = MaxUnpooling2D(pool_size)([conv_24, mask_1])\n",
    "\n",
    "    conv_25 = Convolution2D(64, (kernel, kernel), padding=\"same\")(unpool_5)\n",
    "    conv_25 = BatchNormalization()(conv_25)\n",
    "    conv_25 = Activation(\"relu\")(conv_25)\n",
    "\n",
    "    conv_26 = Convolution2D(n_labels, (1, 1), padding=\"same\")(conv_25)\n",
    "    conv_26 = BatchNormalization()(conv_26)\n",
    "   \n",
    "    conv_26  = Reshape((n_labels,-1))(conv_26 )\n",
    "\n",
    "    conv_26  = Permute((2,1))(conv_26 )\n",
    "    outputs = Activation(output_mode)(conv_26)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs, name=\"SegNet\")\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='sgd',metrics=['accuracy'])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SegNet0(): \n",
    "  n_label=2\n",
    "  img_w=256\n",
    "  img_h=256\n",
    "  model = Sequential()  \n",
    "  #encoder  \n",
    "  model.add(Conv2D(64,(3,3),strides=(1,1),input_shape=(img_w,img_h,4),padding='same',activation='relu'))  \n",
    "  model.add(BatchNormalization())  \n",
    "  model.add(Conv2D(64,(3,3),strides=(1,1),padding='same',activation='relu'))  \n",
    "  model.add(BatchNormalization())  \n",
    "  model.add(MaxPooling2D(pool_size=(2,2)))  \n",
    "  #(128,128)  \n",
    "  model.add(Conv2D(128, (3, 3), strides=(1, 1), padding='same', activation='relu'))  \n",
    "  model.add(BatchNormalization())  \n",
    "  model.add(Conv2D(128, (3, 3), strides=(1, 1), padding='same', activation='relu'))  \n",
    "  model.add(BatchNormalization())  \n",
    "  model.add(MaxPooling2D(pool_size=(2, 2)))  \n",
    "  #(64,64)  \n",
    "  model.add(Conv2D(256, (3, 3), strides=(1, 1), padding='same', activation='relu'))  \n",
    "  model.add(BatchNormalization())  \n",
    "  model.add(Conv2D(256, (3, 3), strides=(1, 1), padding='same', activation='relu'))  \n",
    "  model.add(BatchNormalization())  \n",
    "  model.add(Conv2D(256, (3, 3), strides=(1, 1), padding='same', activation='relu'))  \n",
    "  model.add(BatchNormalization())  \n",
    "  model.add(MaxPooling2D(pool_size=(2, 2)))  \n",
    "  #(32,32)  \n",
    "  model.add(Conv2D(512, (3, 3), strides=(1, 1), padding='same', activation='relu'))  \n",
    "  model.add(BatchNormalization())  \n",
    "  model.add(Conv2D(512, (3, 3), strides=(1, 1), padding='same', activation='relu'))  \n",
    "  model.add(BatchNormalization())  \n",
    "  model.add(Conv2D(512, (3, 3), strides=(1, 1), padding='same', activation='relu'))  \n",
    "  model.add(BatchNormalization())  \n",
    "  model.add(MaxPooling2D(pool_size=(2, 2)))  \n",
    "  #(16,16)  \n",
    "  model.add(Conv2D(512, (3, 3), strides=(1, 1), padding='same', activation='relu'))  \n",
    "  model.add(BatchNormalization())  \n",
    "  model.add(Conv2D(512, (3, 3), strides=(1, 1), padding='same', activation='relu'))  \n",
    "  model.add(BatchNormalization())  \n",
    "  model.add(Conv2D(512, (3, 3), strides=(1, 1), padding='same', activation='relu'))  \n",
    "  model.add(BatchNormalization())  \n",
    "  model.add(MaxPooling2D(pool_size=(2, 2)))  \n",
    "  #(8,8)  \n",
    "  #decoder  \n",
    "  model.add(UpSampling2D(size=(2,2)))  \n",
    "  #(16,16)  \n",
    "  model.add(Conv2D(512, (3, 3), strides=(1, 1), padding='same', activation='relu'))  \n",
    "  model.add(BatchNormalization())  \n",
    "  model.add(Conv2D(512, (3, 3), strides=(1, 1), padding='same', activation='relu'))  \n",
    "  model.add(BatchNormalization())  \n",
    "  model.add(Conv2D(512, (3, 3), strides=(1, 1), padding='same', activation='relu'))  \n",
    "  model.add(BatchNormalization())  \n",
    "  model.add(UpSampling2D(size=(2, 2)))  \n",
    "  #(32,32)  \n",
    "  model.add(Conv2D(512, (3, 3), strides=(1, 1), padding='same', activation='relu'))  \n",
    "  model.add(BatchNormalization())  \n",
    "  model.add(Conv2D(512, (3, 3), strides=(1, 1), padding='same', activation='relu'))  \n",
    "  model.add(BatchNormalization())  \n",
    "  model.add(Conv2D(512, (3, 3), strides=(1, 1), padding='same', activation='relu'))  \n",
    "  model.add(BatchNormalization())  \n",
    "  model.add(UpSampling2D(size=(2, 2)))  \n",
    "  #(64,64)  \n",
    "  model.add(Conv2D(256, (3, 3), strides=(1, 1), padding='same', activation='relu'))  \n",
    "  model.add(BatchNormalization())  \n",
    "  model.add(Conv2D(256, (3, 3), strides=(1, 1), padding='same', activation='relu'))  \n",
    "  model.add(BatchNormalization())  \n",
    "  model.add(Conv2D(256, (3, 3), strides=(1, 1), padding='same', activation='relu'))  \n",
    "  model.add(BatchNormalization())  \n",
    "  model.add(UpSampling2D(size=(2, 2)))  \n",
    "  #(128,128)  \n",
    "  model.add(Conv2D(128, (3, 3), strides=(1, 1), padding='same', activation='relu'))  \n",
    "  model.add(BatchNormalization())  \n",
    "  model.add(Conv2D(128, (3, 3), strides=(1, 1), padding='same', activation='relu'))  \n",
    "  model.add(BatchNormalization())  \n",
    "  model.add(UpSampling2D(size=(2, 2)))  \n",
    "  #(256,256)  \n",
    "  model.add(Conv2D(64, (3, 3), strides=(1, 1), input_shape=(4,img_w, img_h), padding='same', activation='relu'))  \n",
    "  model.add(BatchNormalization())  \n",
    "  model.add(Conv2D(64, (3, 3), strides=(1, 1), padding='same', activation='relu'))  \n",
    "  model.add(BatchNormalization())  \n",
    "  model.add(Conv2D(n_label, (1, 1), strides=(1, 1), padding='same'))  \n",
    "  model.add(Reshape((n_label,img_w*img_h)))  \n",
    "  #axis=1和axis=2互换位置，等同于np.swapaxes(layer,1,2)  \n",
    "  model.add(Permute((2,1)))  \n",
    "  model.add(Activation('softmax'))  \n",
    "  model.compile(loss='categorical_crossentropy',optimizer='sgd',metrics=['accuracy'])  \n",
    "  model.summary()  \n",
    "  return model  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(key='SegNet',EPOCHS = 10,BatchSize = 4,train_numb_per_epoch = 10*8,valid_rate = 0.2,img_w = 256,img_h = 256): \n",
    "    EPOCHS = EPOCHS\n",
    "    BS = BatchSize\n",
    "    img_w = img_w  \n",
    "    img_h = img_h\n",
    "\n",
    "    train_numb=train_numb_per_epoch*EPOCHS\n",
    "    valid_numb = train_numb*valid_rate\t\n",
    "\n",
    "    method = {\n",
    "    \"fcn32\": FCN32,\n",
    "    #\"fcn8\": FCN8.FCN8,\n",
    "    'SegNet': SegNet,\n",
    "    'SegNet0': SegNet0,\n",
    "    #'unet': UNet.UNet\n",
    "    }\n",
    "    m = method[key]()\n",
    "\n",
    "    m.compile(loss='categorical_crossentropy',optimizer=\"adadelta\",metrics=['acc'])\n",
    "    \n",
    "    modelcheck = ModelCheckpoint('D:\\Python\\seg-data/model/%s_model.h5' % key,#modelcheck = ModelCheckpoint('..\\..\\Python\\seg-data/model/SegNet-'+time.strftime(f'%Y-%m-%d-%a-%H-%M-%S',time.localtime(time.time()))+'.h5',\n",
    "                                 monitor='val_acc',\n",
    "                                 save_best_only=True,\n",
    "                                 mode='max')  \n",
    "    tb=TensorBoard(log_dir='D:\\Python\\seg-data/log/%s_log/' % key)\n",
    "    callableTF = [modelcheck,tb]   \n",
    "\n",
    "    print (\"the number of train data is\",train_numb,train_numb//BS)  \n",
    "    print (\"the number of val data is\",valid_numb,valid_numb//BS)\n",
    "    H = m.fit_generator(generator=generateDataTF(BS,img_w,img_h,2,['test.tif'],['test_label.png']),\n",
    "                            steps_per_epoch=train_numb_per_epoch,\n",
    "                            epochs=EPOCHS,\n",
    "                            verbose=0,\n",
    "                            validation_data=generateDataTF(BS,img_w,img_h,2,['test.tif'],['test_label.png']),\n",
    "                            validation_steps=train_numb_per_epoch*valid_rate,\n",
    "                            callbacks=callableTF,\n",
    "                            max_q_size=1)  \n",
    "\n",
    "    # plot the training loss and accuracy\n",
    "    plt.style.use(\"ggplot\")\n",
    "    plt.figure()\n",
    "    N = EPOCHS\n",
    "    plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n",
    "    plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n",
    "    plt.plot(np.arange(0, N), H.history[\"acc\"], label=\"train_acc\")\n",
    "    plt.plot(np.arange(0, N), H.history[\"val_acc\"], label=\"val_acc\")\n",
    "    plt.title(\"Training Loss and Accuracy on %s Satellite Seg\" % key)\n",
    "    plt.xlabel(\"Epoch #\")\n",
    "    plt.ylabel(\"Loss/Accuracy\")\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    plt.savefig(\"D:\\Python\\seg-data/model/%s plot.png\"% key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 256, 256, 64)      2368      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256, 256, 64)      256       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 256, 256, 64)      36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 256, 256, 64)      256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 128, 128, 128)     73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 128, 128, 128)     512       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 128, 128, 128)     147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 128, 128, 128)     512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 64, 64, 256)       295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 64, 64, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 64, 64, 256)       590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 64, 64, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 64, 64, 256)       590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 64, 64, 256)       1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 32, 32, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 32, 32, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 32, 32, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 32, 32, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 32, 32, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 32, 32, 512)       2048      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 16, 16, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 16, 16, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 16, 16, 512)       2048      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 16, 16, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 16, 16, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 16, 16, 512)       2048      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 32, 32, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 32, 32, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 32, 32, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 32, 32, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 32, 32, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 32, 32, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 32, 32, 512)       2048      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 64, 64, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 64, 64, 256)       1179904   \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 64, 64, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 64, 64, 256)       590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 64, 64, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 64, 64, 256)       590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 64, 64, 256)       1024      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2 (None, 128, 128, 256)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 128, 128, 128)     295040    \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 128, 128, 128)     512       \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 128, 128, 128)     147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 128, 128, 128)     512       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_5 (UpSampling2 (None, 256, 256, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 256, 256, 64)      73792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 256, 256, 64)      256       \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 256, 256, 64)      36928     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 256, 256, 64)      256       \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 256, 256, 2)       130       \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 2, 65536)          0         \n",
      "_________________________________________________________________\n",
      "permute_1 (Permute)          (None, 65536, 2)          0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 65536, 2)          0         \n",
      "=================================================================\n",
      "Total params: 31,821,442\n",
      "Trainable params: 31,804,546\n",
      "Non-trainable params: 16,896\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From c:\\users\\chao\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "the number of train data is 8000 1000\n",
      "the number of val data is 1600.0 200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\chao\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:37: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(generator=<generator..., steps_per_epoch=80, epochs=100, verbose=0, validation_data=<generator..., validation_steps=16.0, callbacks=[<keras.ca..., max_queue_size=1)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen-Sub-Image-Data...gen-Sub-Image-Data...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train(key='SegNet0',EPOCHS = 100,BatchSize = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
